queue: net_surgery
       pascal-multilabel-with-datalayer
        


####################
Contents::
####################
setups
thoughts
lmdbs
  creating lmdb
    preparation
    run
caffe tuts
  Blobs, Layers, Nets
  forward/backward
  loss
  solver
  training tips
  layer catalogue
    vision layers
    loss layers
    activation/neuron layers
    data layers
    common layers
  interfaces
  data
  caffeinated convolution
readme_notebooks (caffe/examples)
  classification
    visualizing layers
  solving with lenet
    generating prototexts
  fine-tuning
  logreg
    hdf5
    NetSpec
prototexts
  faster
  caffe
caffe
  functions
    set_mode_cpu
    set_mode_gpu
  classes
    Layers
    Net
      constructor
      instance vars
      functions
    SGDSolver
  variables
    TEST
modules
  io
    classes
      Transformer
    functions
    variables
  proto
other notes

scratch

::||;;''//





####################
setups::
####################
run code:
  import numpy as np
  import matplotlib.pyplot as plt
  %matpltlib inline                #for ipython notebook plotting inline??
  

  #set pyplot params
  plt.rcParams['figure.figsize']  = (10,10)
  plt.rcParams['image.interpolation'] = 'nearest'
  plt.rcParams['image.cmap'] = 'gray' 
  
  import caffe
  
have a model:
  can download:
    ex:
      caffe scripts/download_model_binary.py models/bvlc_reference_caffenet



####################
thoughts::
####################
import caffe
make a net
grab data and put into the net
(solver.net.blobs['data'].data = numpy array of data
call solver.step
put new data, step again, etc

NOTE:initializing net: if names match, use params, otherwise, use the weight filler





####################
lmdbs::
####################
http://stackoverflow.com/documentation/caffe/5344/prepare-data-for-training/19019/prepare-image-dataset-for-image-classification-task#t=201608091243456475019

make -j8
make test -j8
make tools [-j8??]
______________________________
creating lmdb||
  caffe/build/tools/convert_imageset

  ______________________________
  preparation;;
    images into a folder
    labels text file, 1 line per input image
      ex:
        00000.jpg label1
        00001.jpg label2
        00002.jpg label3

  ______________________________
  run;;
    GLOG_logtostderr=1 $CAFFE_ROOT/build/tools/convert_imageset \
      --resize_height=h --resize_width=w --shuffle \
      /path/to/jpegs \
      /path/to/labels/train.txt \
      /path/to/lmdb/train_lmdb

    GLOG_logtostderr: log to stderr
    resize: resize all images to these dimensions
    shuffle: randomly order the images
    --backend: allows choice between lmdb and levelDB
    --gray: grayscale
    --encoded: keep encoded form in database
    --encoded_type: jpg/png
    --help: shows help
  





####################
caffe tuts::
####################
______________________________
Blobs, Layers, Nets||
  Blobs: array wrapper: contains all the data
    holds: data
             (blob.data)
             the data passed to the next layer
           image batches
             stored as: NKHW
             N = index of image in batch
             K = Kth channel
             H = height (row)
             W = width (col)
           model params
             varies:
             ex: 96 filters of 11x11, 3 channels:
               96x3x11x11
             ex: fc, 1024 in, 1000 out:
               1000 x 1024
             
           derivatives
             (blob.diff)
  layers contain blobs
    setup: initialize layer 
    forward: forward pass
    backward: backward pass
    has CPU and GPU versions of routines

  for fine-tuning, use
  net.copy_from(pretrained_model) ???


  nets contain layers
    the prototexts
    ex:
      name: "LogReg"
      layer {
        name: "mnist"
        type: "Data"
        top: "data"
        top: "label"
        data_param {
          source: "input_leveldb"
          batch_size: 64
        }
      }
      layer {
        name: "ip"
        type: "InnerProduct"
        bottom: "data"
        top: "ip"
        inner_product_param {
          num_output: 2
        }
      }
      layer {
        name: "loss"
        type: "SoftmaxWithLoss"
        bottom: "ip"
        bottom: "label"
        top: "loss"
      }
    Net::Init() initializes net by reading prototexts
      links layers
      handles layer calls etc
    
______________________________
forward/backward||
  Net.Forward: runs overall forward
  Net.Backward: runs overall backward
  Layer.Forward: computes
  Layer.Backward: computes

  Solver optimizes: calls forward then backwards,
  then minimizes loss


______________________________
loss||
  loss function... minimize for optimization
  ex layer:
    layer {
      name: "loss"
      type: "SoftmaxWithLoss"
      bottom: "pred"
      bottom: "label"
      top: "loss"
    }
  loss weights: relative importance of different
    loss layers if multiple exist
  end with Loss = loss layer
  add in loss_weight = nonzero to include a non-default
    loss layer in loss function, loss_weight is for
    top blobs

  final loss = total weighted losses over entire network

______________________________
solver||
  Solver: does optimization
  coordinates forward and backward
  include:
    SGD
      type: "SGD"
      descr:
        linear combo of negative gradient and previous
        update
        learning rate alpha = weight of negative gradient
        momentum weight mu  = weight of previous gradient
        Vt+1 = mu*Vt - alpha*grad
        Wt+1 = Wt + Vt+1
      rules of thumb
        alpha: 0.01, drop by a constant factor (10)
        mu = 0.9
        mu and alpha should have inverse relationship
      ex:
        base_lr: 0.01
        lr_policy: step #how to drop learning rate
        gamma: 0.1      #drop learning rate by this factor
        stepsize: 100000#when to drop learning rate
        max_iter: 350000#when to end
        momentum: 0.9
    AdaDelta
      type: "AdaDelta"
      vt(i) = (rms((vt-1)(i)) / rms(grad(i))grad(i)
      RMS(grad) = sqrt(E[g^2] + eps)
      E[g^2]t = deltaE[g^2]t-1 + (1-sigma)g^2t...
      Wt+1 = Wt - alpha(vt)
    AdaGrad
      type: "AdaGrad"
      Wt+1 = Wt - alpha(grad / rms(grad))
    Adam
      type: "Adam"
      mt = b1(mt - 1) + (1 - b1)(grad)
      vt = b2(vt - 1) + (1 - b2)(grad)
      wt+1 = wt - alpha * sqrt((1 - b2^t) / 1-(b1^t)) * (mt / sqrt(vt) + eps)
      rules of thumb:
      momentum = beta1 = 0.9
      momentum2 = beta2 = .999
      delta = eps = e-8
    Nesterov
      type: "Nesterov"
      uses convexity so might not work for all
      Vt+1 = mu*Vt - alpha(grad(Wt + mu * vt))
      Wt+1 = Wt + Vt+1
    RMSProp
      type: "RMSProp"
      MS((Wt)i) = delta*(MS((Wt-1)i) + (1-delta)(grad(Wt))^2
      Wt+1  Wt - alpha*(gradW / sqrt(MS(Wt)))
  tasks:
    create training/testing networks
    optimizes iteratively
    evaluates test
    snapshots during optimization
    
  per iteration:
    network forward, compute output, loss
    network backward, compute grads
    incorp grads into param updates
    update solver state (lr, history, method, etc)
    
  Scaffolding:
    Solver::Presolve()
      initialize net and optimization method

  updates:
    Solver::ComputeUpdateValue()
      incorporates weight decay
      multiplies error gradient
      multiplies learning rate alpha
      update stored in diff
      calls Blob::Update

  Snapshots:
    Solver::Snapshot()
    Solver::SnapshotSolverState()
    
    snapshots weights and solver state

    Solver::Restore()
    Solver::RestoreSolverState()

    restores solver and weights to resume training

    snapshot: snapshotinterval
    snapshot_prefix: "/path/to snapshots"
    snapshot_diff: false (saves differences too for debugging)
    snapshot_after_train: true (saves final model, true by default) 
______________________________
training tips||
  loss = Nan, large, or infinity: training diverged
  use smaller learning rate

______________________________
layer catalogue||
  ______________________________
  vision layers;;
    input: image
    output: image
    ______________________________
    convolution
    Layer type: Convolution
    CPU: src/caffe/layers/convolution_layer.cpp
    GPU: src/caffe/layers/convolution_layer.cu
    params:
      req:
        num_output (c_o): # of filters
        kernel_size (kernel_h, kernel_w): height/width of filters
      rec:
        weight_filler default[type: 'constant' value: 0]
      opt:
        bias_term: default[true]: use additive biases too?
        pad (pad_h, pad_w): default[0]: # of pixels to pad
        stride (stride_h, stride_w): default[1]: stride
        group (g) default[1]: >1 = separate i/o channels
                              into groups, separately filtered

        






####################
readme_notebooks (caffe/examples)::
####################

______________________________
classification||
  get pretrained model:
  ./scripts/download_model_binary.py models/bvlc_reference_caffenet
  
  create a caffe.Net object
    net = caffe.Net(prototxt, caffemodel, caffe.TEST)

  set_mode_cpu()
  set_mode_gpu()
  set_device(gpu_id_number)


  can reshape batch input using
    Net.blobs['data'].reshape(batchsize,depth/channels,width,height)
  
  put data into the net
    net.blobs['data'].data[...] = transformer.preprocess('data', caffe.io.load_image(caffe_root+'catjpg'))

    NOTE:
      net.blobs['data'].data[:,:,:,:] is equivalent to the above left hand side
      transformer.preprocess etc = just the image
      data format: 4 dimensions: batchsize, channels, height, width
        channels may be in bgr, rgb, rgbd, blah etc, look at the net



  forward pass:
    out=net.forward()
    probabilities = out['prob'][imagenum_in_the_batch]
    probabilities = net.blobs['prob'].data[imagenum_in_the_batch]

  layers are in net (ordered dict)

  ______________________________
  FOR ACTIVATIONS->

    for layer_name, blob in net.blobs.iteritems():
       blob.data = the activations (output data)
  ______________________________
  for weights->
    for layer_name, param in net.params.iteritems():
      param[0]: conv filter
        conv shapes: outputchannels, inputchannels, height, width
          outputchannels is like... # of filters?
          does that mean filters are run on all input channels and combined?
          outputchannels is a multiple of input channels?
          each channel gets its own filter??
          NOTE:
            shape of param[0].data is:
            (outputchannels, input channels, height, width)
            this implies each input channel gets its own output channel per channel
            (ie: inputs: 3 channels rgb, outputs: 84,
            actual number of parameters o 3*84*filterdimensions
            then output size ~ 3 * 84 * image dimensions
            but looking a the data sizes... convolution is a 2-d convolution in 3 dimensions?
            (for each input channel have a filter
            result of the filter = convolution across channels...??)
      param[1]: the biases
      param[i].data: the actual matrices
        bias shapes: same as # of output channels
      fc


  
  ______________________________
  visualizing layers;;
      #bring data into range [0,1]
      def vis_square(data, padsize=1, padval=0):
        data -= data.min()
        data /= data.max()
          #make # of filters square
        n = int(np.ceil(np.sqrt(data.shape[0])))
        padding = ((0, n ** 2 - data.shape[0]), 
                   (0, padsize), (0, padsize))
                 + ((0, 0),) * (data.ndim - 3)
        data = np.pad(data, padding, mode='constant',
                      constant_values=(padval, padval))
    
        #tile filter into image
        data=data.reshape((n,n) + data.shape[1:]).transpose(
                         (0,2,1,3) + tuple(range(4,data.ndim + 1)))
        data=data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])
        plt.imshow(data)
      filters=net.params['conv1'][0].data
      vis_square(filters.transpose(0,2,3,1))

      feat = net.blobs['conv1'].data[0, :36]
      vis_square(feat, padval=1)

      filters=net.params['conv2'][0].data
      vis_square(filters[:48].reshape(48*2,5,5))

      .....
      check most likely labels

      #file with the labels
      imagenet_labels_filename = caffe_root + 'data/ilsvrc12/synset_words.txt'
      
      #load labels file
      try:
          labels=np.loadtxt(iamgenet_labels_filename, str, delimiter='\t')
      except:
          !../data/ilsvrc12/get_ilsvrc_aux.sh
          labels = nploadtxt(imagenet_labels_filename, str, delimiter='\t')

      #get top 5 labels and then print
      top_k = net.blobs['prob'].data[0].flatten().argsort()[-1:-6:-1]
      print labels[top_k]


______________________________
solving with lenet||
  import os
  os.chdir('..')
  import sys
  sys.path.insert(0, './python')
  import caffe
  from pylab import *

  download databases:
  data/mnist/get_mnist.sh
  examples/mnist/create_mnist.sh


  ______________________________
  GENERATING PROTOTEXTS;;
  python file------------------------------
    from caffe import layers as L
    from caffe import params as P
    
    def lenet(lmdb, batch_size):
        # our version of LeNet: a series of linear and simple nonlinear transformations
        n = caffe.NetSpec()
        n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,
                                 transform_param=dict(scale=1./255), ntop=2)
        # NOTE: scale=1./255 scales the range of the image values to [0,1]
        n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))
        n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)
        n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))
        n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)
        n.ip1 = L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))
        n.relu1 = L.ReLU(n.ip1, in_place=True) (bottom blob and output blob the same(mutates the data)
                                                to reduce memory usage)
        n.ip2 = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))
        n.loss = L.SoftmaxWithLoss(n.ip2, n.label)
        return n.to_proto()
        
    with open('examples/mnist/lenet_auto_train.prototxt', 'w') as f:
        f.write(str(lenet('examples/mnist/mnist_train_lmdb', 64)))
        
    with open('examples/mnist/lenet_auto_test.prototxt', 'w') as f:
        f.write(str(lenet('examples/mnist/mnist_test_lmdb', 100)))
  ------------------------------
  NOTE: xavier initialization: initializes weights so variances are normalized
  means signals should not disappear/explode as data moves through nets on initialization



  solver = caffe.SGDSolver('filenamepath')

  solver.net = the net
  (can do solver.net.params for weights
          solver.net.blobs for activations)
          
  solver.net = training net
  solver.test_nets = testing netS (can have more than one, index 0 if just 1...???)

  NOTE:
  training iteration:
    solver.forward: forward pass
    solver.backward: backward pass
    solver.update: update the params

  solver.step(num_iterations): run num_iterations iterations of loop above
  solver.solve: goes to max_iter

  can check that nets are loaded and with data by doing:
    solver.nettype[index maybe].blobs['data'].data
    solver.nettype[index maybe].blobs[label'].data
      nettype = train_net or test_nets[i]
  and checking the values

  python code------------------------------
    %%time
    niter = 200
    test_interval = 25
    # store data about training process
    # training loss per iteration
    train_loss = zeros(niter)
    # testing accuracy for each test ceil(niter / test_interval) is # of tests performed
    test_acc = zeros(int(np.ceil(niter / test_interval)))
    # outputs for 1st 8 images per iteration
    output = zeros((niter, 8, 10))
    
    # the main solver loop
    for it in range(niter):
        solver.step(1)  # SGD by Caffe
        
        # store the train loss
        train_loss[it] = solver.net.blobs['loss'].data
        
        # store the output on the first test batch
        # (start the forward pass at conv1 to avoid loading new data)
        solver.test_nets[0].forward(start='conv1')
        output[it] = solver.test_nets[0].blobs['ip2'].data[:8]
        
        # run a full test every so often
        # (Caffe can also do this for us and write to a log, but we show here
        #  how to do it directly in Python, where more complicated things are easier.)
        if it % test_interval == 0:
            print 'Iteration', it, 'testing...'
            correct = 0
            for test_it in range(100):
                solver.test_nets[0].forward()
                correct += sum(solver.test_nets[0].blobs['ip2'].data.argmax(1)
                               == solver.test_nets[0].blobs['label'].data)
            test_acc[it // test_interval] = correct / 1e4
    from caffe.proto import caffe_pb2
      s = caffe_pb2.SolverParameter()                       # create solver parameters
      s.random_seed = 0xCAFFE                               # random seed for rng
      s.train_net = train_net_path                          # where is the train net
      s.test_net.append(test_net_path)                      # where is the test net
      s.test_interval = 500                                 # how often to test
      s.test_iter.append(100)                               # number to test
      s.iter_size                                           # size of a single iteration
                                                              effectively boosts batch size without affecting
                                                              memory usage
      s.max_iter                                            # when to stop
      s.type = "SGD"                                        # 'SGD', 'Adam', 'Nesterov', 'AdaGrad', RMSProp, etc: see caffe tuts/solver
      s.base_lr = 0.01                                      # base learning rate
      s.momentum = 0.9                                      # momentum component
      s.weight_decay = 5e-4                                 # weight decay
      s.lr_policy = 'inv'                                   # learning policy, see lr_policy in prototexts/faster
      s.gamma = 0.0001                                      # gama for lr policy
      s.power = 0.75                                        # for lr_policy

      s.stepsize = 20000                                    # stepsize for lr_policy if lr_policy = 'step'
      s.display = 1000                                      # display progress very n iterations
      s.snapshot = 5000                                     # record a snapshot every 5000 iterations
      s.snapshot_prefix = 'mnist/custom_net'                # where to save snapshots
      s.solver_mode = caffe_pb2.SolverParameter.GPU         # what to use to solve
                      caffe_pb2.SolverParameter.CPU
      with open(solver_config_path, 'w') as f:              # save the solver.prototext file
        f.write(str(s))


    solver = caffe.get_solver(solver_config_path)             #open the solver

    for iterations:
        solver.step(1) loop
        do stuff


  ------------------------------

______________________________
fine-tuning||
  create the model...
  base_lr, lr_mult, decay_mult etc should be set (see caffe/classes/layers)

  generate solver
  solver.net.copy_from(weights) for fine-tuning...???
  not solver.net = caffe.NET(prototxt, weights caffe.TRAIN)??
    caffe.NET is only for if we want only net to run testing...
    there is a caffe.TRAIN but... what to do with it?


______________________________
logreg||
  ______________________________
  HDF5: import h5py;;
  creates the imageset file (as hdf5)

    comp_kwargs = {'compression':'gzip', 'compression_opts':1}??
    with h5py.File(fname, 'w') as f:
      f.create_dataset('data', data=data, **comp_kwargs)
      f.create_dataset('label', data=labels, **comp_kwargs)
      f['name'] = pythonvalues
  lists the imageset file
    with open('train.txt') as f:
      f.write(hdf5file.h5 + '\n') etc...

  ______________________________
  NetSpec;;
    def logreg(hdf5file, batch_size):
      n = caffe.NetSpec()
      n.data, n.label = L.HDF5Data(batch_size = batch_size, source=hdf5file, ntop = 2)
      n.ip1 = L.InnerProduct(n.data, num_output=2, weight_filler=dict(type='xavier'))
      n.accuracy = L.Accuracy(n.ip1, n.label)
      n.loss = L.SoftmaxWithLoss(n.ip1, n.label)
      return n.to_proto()
    with open(fname.proto) as f:
      f.write(str(logreg(hdf5file, batch_size)))

  ______________________________
  solver spec: see fine-tuning: caffe_pb2.SolverParameter
    
  
  

  

####################
prototexts::
####################
NOTE: see solving with lenet/generating prototexts, and sample python code at end of lenet

3 types:
solver
  NOTE: faster: = faster has
        caffe = only caffe
  faster:
    train_net: "path_to_train.pt": from root
    base_lr: n
      the base learning rate is n
    momentum: [0,1]
      the momentum for SGD
    weight_decay: [0,1]
      decay of weights if there were no
      data
    lr_policy:
      fixed: base_lr doesn't change
      step: base_lr * gamma ^ (floor(iter / stepsize))
      exp: base_lr * gamma^iter
      inv: base_lr * (1 + gamma*iter) ^ (-power)
      multistep: step, but with non-uniform steps
                 defined by stepvalue
      poly: (1 - iter/max_iter)^(power)
      sigmoid: 1/(1+exp(-gamma*(iter-stepsize)))
    gamma: [0,1]
      multiply learning rate by a factor of gamma
      every stepsize iterations
    stepsize:n
      drop learning rate every n iterations
    power: [0,1]
      i forget
    display: n
      display every n iterations
    

  caffe:
    max_iter: n
      train for n iterations
    test_net: "path to test.pt/prototext etc"
      the test net spec
    test_iter: 100
      forward passes for the test
    test_interval: n
      do testing every n training iterations
    snapshot: n
    snapshot_prefix: "prefix"    
    


train
test



####################
caffe::
####################
______________________________
functions||
  ______________________________
  set_mode_cpu();;
    uses cpu for the calculations
  ______________________________
  set_mode_gpu();;
    uses gpu for the calculations
  ______________________________
  set_device(num);;
    params:
      num: the gpu number to use
    description:
      chooses which gpu to use, just use 0
  ______________________________
  SGDSolver(fname_from root);;
    creates a SGD solver object
    takes the solver prototxt



______________________________
classes||
  ______________________________
  Layers;;
    from caffe import layers as L
    L.Convolution(input, kernel_size=n, stride=k, num_output=n, pad=pad, group=group,
                         param=param,  weight_filler=dict(type='xavier'), bias_filler=blah)
      input: input layer
      kernel_size: size of filters
      stride: stride of filter
      num_output: # of filters
      pad: padding the image
      
      weight_filler: how to initialize weights?

    L.Pooling(input, kernel_size, stride=stride, pool=P.Pooling.MAX)
      input: input layer
      kernel_size: windowsize for pooling
      stride: stride on the pooling
      pool: how to pool?

    L.Dropout(input, in_place=True)

    L.SoftmaxWithLoss(input, labels): training

    L.Softmax(input): testing

    L.ImageData(transform_param=dict(mirror=trainn, crop_size=227, mean_file=caffe_root+'data/ilsvrc12/imagenet_mean.binaryproto'),
                source=files.txt, batch_size=50, new_height=256, new_width=256, ntop=2)


    L.Accuracy(net_outputlayer, labels)


    L.DummyData(shape=dict(dim=[batch, channels, height, width]))

    L.InnerProduct(input, num_output=n, weight_filler=dict(type='xavier'), bias_filler=dict(stuff))
      input: input layer
      num_output: # of outputs
      weight_filler: how to initialize weights
                     {type:'xavier'}, {type:'gaussian', std:0.005}, {type:'constant', value=0.1}
      bias_filler: see weight_filler
      
    L.ReLU(input, inplace=True/False)
      input: input layer
      inplace: who knows?

    L.ELU?

    L.Sigmoid?

    L.SoftmaxWithLoss(scores, labels)
      scores: net's outut scores
      labels: correct labels
    ...

  ______________________________
  Net;;
    ______________________________
    constructor''
      Net(model_def, model_weights, mode)
        model_def: downloaded models, the deploy.prototxt file path
        model_weights: downloaded models, the .caffemodel file
        mode: run in test mode or training mode etc
              values:
                caffe.TEST

        NOTE: model_def is probably the test prototexts
    ______________________________
    instance vars''
      blobs:
        dictionary containing all the layers' outputs?
        'data' : input
        'convi': conv layer
        'pooli': pool layer
        'fci'  : fully connected layer
        'prob' : 
          .data: the output data for this layer
            dimensions: nth image in batch
                        depth
                        width
                        height
      params:
        dictionary of parameters
        'convi': convi layer's params
        'fci'  : fci layer's params
          [0].data: layer params
          dimensions: [numfilter, depth, height, width]
    ______________________________
    functions''
      forward(start = 'layername')
        runs a forward pass, giving the results
      
      backward()
        runs a backward pass

      update()
        update the weights



______________________________
  NOTE:
    SGDSolver
    AdaDelta
    Adam
    are all solvers...
    should similarly all have nets etc
  ______________________________
  SGDSolver;;
    ______________________________
    var:
      net.blobs
    notes:
      blobs: all the outputs
        keys: 'data', layer names, 'loss', 'label'
        objs: array of output vals
    ______________________________
    var:
      net.params
    notes:
      params: dictionary of parameters
        keys: layer names
        objs: array: [0].data = weights
                     [1].data = biases?
                     [0].diff = updates
                     
    ______________________________
    func:
      net.forward()
    desc:
      does a single forward pass

    ______________________________
    func:
      test_nets[i].forward()
    desc:
      runs the test net index i
      outputs a dictionary:
        {'loss': array(lossval, dtype)}
    ______________________________
    func:
      step(i)
    desc:
      runs a loop of forward, backward, 
      update for i iterations
      i: # of steps to take
    
      

______________________________
variables||
  ______________________________
  TEST;;
    variable for 






####################
modules::
####################
______________________________
io||
  ______________________________
  classes;;
    ______________________________
    Transformer''
      ______________________________
      constructor
        Transformer(dict);
          dict: a dictionary
                shoud contain the following entries:
                  'data':net.blobs['data'].data.shape
                  net = a caffe.Net object

      

______________________________
proto;;
  example code: see end of lenet section

      



####################
other notes::
####################
the mean image for image centering is at
caffe/python/caffe/imagenet/ilsvrc_2012_mean.npy
  (for some reason, it's int he format of [bgr, x, y]
   cv2 can't read it)

net: contains architecture
solver: learning hyperparameters


####################
scratch::
####################
layers??-what is it really called?
some kind of array...
layer = [0].data?

data: matrices containing data/params etc
      gives the layer shape
