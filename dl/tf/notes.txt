https://github.com/uber/horovod#usage
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/resnet50/resnet50.py
https://www.tensorflow.org/guide/tensors
keras: build advanced models

contents
-1- graph
	-1.1- tensors
	-1.4- tf.data
	-1.2- operations
	-1.3- layers
-2- running
-3- visualization
-4- training
	-4.1- get data
	-4.2- get model
	-4.3- get error func
	-4.4- train
-5- keras
-6- distributed tf

______________________________
-1- graph
	tf.Graph: a graph
	tf.Operation: nodes of graph
	tf.Tensor: edges of graph

	______________________________
	-1.1- tensors
		properties:
			type:
				tf.float32
				np.dtype
			shape:
				tf.TensorShape
				np.shape
				list/tuple
			an input position that will be provided at runtime
			passed through feed_dict parameter
		methods:
			tf.rank(tensor)
			   # of dimensions
			indexing like numpy arrays
			shape
				operation to return the shape at runtime
			tf.reshape
				np.reshape
			tf.cast(tensor, type)
				like np.array(arr, type)
			eval(feed_dict = None)
				evaluate with default session

		types:
			tf.Variable
			tf.constant(val, dtype)
			tf.placeholder(type, shape)
			tf.SparseTensor

	______________________________
	-1.4- tf.data
		______________________________
		tf.data.Dataset
			seq of elements, each is one or more tensors
			abstraction for groups of tensors

			from_tensors()
			from_tensor_slices():
				create dataset from tf.Tensor objects

			tf.data.TFRecordDataset(fname or [fnames...])
				can use placeholder to make initializers

			output_types
				tf.DType: tensor data type
			output_shapes
				tf.TensorShape: a shape for the data

			______________________________
			transformations (tf.dataDataset)
				batch()
				map()
				repeat

		______________________________
		tf.data.Iterator
			abstraction for iterator over datasets
			creation:
				isinstance(x, tf.data.Dataset) == True
				x.make_one_shot_iterator()
				x.make_initializable_iterator()

			get_next():
				returns a tf tensor for next item in iterator
			initializer:
				re-initialize the iterator
			make_initializer(dataset)
				initialize with that dataset

			raises tf.errors.OutOfRangeError
				at end of iterator
				(like python StopIteration)

			types:
				one_shot
					run once, simplest, easiest

				initializable
					can parameterize with a placeholder
					must call run on initializer before using

					ex:
						max_value = tf.placeholder(tf.in64, shape = [])
						dataset = tf.data.Dataset.range(max_value)
						iterator = dataset.make_initializable_iterator()
						nextElement = iterator.get_next()
						sess.run(
							iterator.initializer, feed_dict = {
								max_value: myval})
						#iterator is initialized to go from 0 to myval non-inclusive
				reinitializable
					can be reinitialized with different datasets
					ex:
						it = tf.data.Iterator.from_structure(
							dataset.output_types, dataset.output_shapes)
						n = it.get_next()
						init1 = it.make_initializer(dataset1)
						init2 = it.make_initializer(dataset2)
						sess.run(initX) and then use the iterator
				feedable
					inName = tf.placeholder(tf.string)
					it = tf.data.Iterator.from_string_handle(
						inName, dataset.output_types, dataset.output_shapes)
					nxt = it.get_next()
					name1 = data1.make_one_shot_iterator().string_handle()
					name2 = data2.make_initializable_iterator().string_handle()

					sess.run(nxt, feed_dict = {inName : nameX})

			a dataset
			example:
				mydata = range(10)
				slices = tf.data.Dataset.from_tensor_slices(mydata)
				next_item = slices.make_one_shot_iterator().get_next()

	______________________________
	-1.2- operations
		basic arithmetic operations
		some tf functions can also return operations
		(initializers are operations)

	______________________________
	-1.3- layers
		operation + tensor

		______________________________
		tf.layers
			call them as functions to return a Tensor on the result
			ie
				x = layer
				inval = tf.PlaceHolder(type, shape)
				outval = x(inval)
				sess.run(outval)
			initialization:
				sess.run(tf.global_variables_initializer())
			types:
				Dense

______________________________
-2- running
	tf.session

	sess = tf.Session(feed_dict = None)
		call run method on a tensor to get value

		example:
			>>> a = tf.constant(3.0)
			>>> b = tf.constant(4.)

			>>> sess.run(a)
			3.

			>>> sess.run({'ab' : (a,b), 'total' : (a+b)})
			{'ab' : (3.0, 4.0), 'total' : 7.0}

		call run on an operation to cause a side-effect
			ie: training, initialization, etc

		feed_dict:
			dict of values to be fed, should be of form
			{placeholder : value}

______________________________
-3- visualization
	tensorboard

	writer = tf.summary.FileWriter('.')
	writer.add_graph(tf.get_default_graph())
	writer.flush()

	tensorboard --logdir .

	open localhost:6006#graphs in browser

______________________________
-4- training
	-4.1- get data
		make datasets
	-4.2- get model
		define inputs (placeholders)
		layers
		operations etc
	-4.3- get error func
		tf.losses
			mean_squared_error(labels = y_true, predictions = blah)
	-4.4- train
		tf.train.Optimizer
			tf.train.GradientDescentOptimizer(mul)

______________________________
-5- keras
	tf.keras
	require pyyaml to save models
	default = checkpoint
	also supported = save_format='h5' -> hdf5
	______________________________
	layers
		common params:
			activation
			kernel_initializer
			bias_initializer
			kernel_regularizer
			bias_regularizer
	______________________________
	models
		sequential
			just a chain of layers
				model = tf.keras.Sequential()
				model.add(layers.Dense(64, activation = 'relu'))
				model.add(layers.Dense(64, activation = 'relu'))
				model.add(layers.Dense(10, activation = 'softmax'))
	______________________________
	training
		model.compile(optimizer, loss, metrics)
			optimizer:
				tf.train optimizer instances
					AdamOptimizer
					RMSPropOptimizer
					GradientDescentOptimizer
				loss
					func to minimize:
						mse
						categorical_crossentropy
						binary_crossentropy
						tf.keras.losses.*
				metrics
					for monitoring
					tf.keras.metrics
		model.fit(data, labels, epochs, batch_size, validation_data, steps_per_epoch)
			data: iterable of data to train on
			labels: iterable of labels for data
			batch_size: ...
			validation_data: tuple of (validData, validLabels)
			steps_per_epoch: steps per epoch
		model.evaluate(data, labels, batch_size)
		model.evaluate(tf.data.Dataset, steps)
			evaluate loss given labels
			(find the accuracy)
		model.predict(data, batch_size = 32)
			run prediction on all inputs

______________________________
-6- distributed tf
	cluster: group of jobs (dict of {job:[tasks...]})
	job: group of tasks (jobname: list of tasks)
	task: a single worker (a server)


	clusters
		spec:
			a dict:
				{
				    "jobname1" : [addr1, addr2, addr3...],
				    "jobname2" : [addr1, addr2, addr3...],
				    ...
				}
		pass spec to tf.train.ClusterSpec
	servers
		1 server per task
		each server:
			set of local gpus
			list of other servers for same job
			a tf.Session
		2-server example:
			cluster = tf.train.ClusterSpec(
				{'job' : ['localhost:2222', 'localhost:2223']})
			server0 = tf.train.Server(
				cluster, job_name = 'job', task_index = 0)
			server1 = tf.train.Server(
				cluster, job_name = 'job', task_index = 1)
		session = tf.Session(server.target)
	task identifiers:
		taken from cluster specs
		format:
			/job:{jobname}/task:{task_index}

		with tf.device(task_identifier):
		    do tf stuff

		with tf.Session("grpc://addr:port") as sess:
				sess.run(stuff)


	typical setup:
		"in-graph replication"
			all parameters on a single server
			have a parameter job and worker job
			parameter job: do stuff with parameters
			worker job: use paramters to run/derive and
			            send update to parameter server
		"between-graph replication"
			build similar graphs, use
			tf.train.replica_device_setter
			1 param server : 1 worker server
			use tf.train.SyncReplicasOptimizer
				to sync params across param servers

			SyncReplicasOptimizer
				ps (parameter server):
					accumulator per variable
					accumulator averages gradients
					apply gradients
					increment global step after all updated
					push global step to tocken_queue
				replicas:
					fetch vars and compute
					push to gradient accumulators
					pop from global step queue
					update local step
					start batch

				example usage:
					from an optimizer:
						opt = something
						opt = tf.train.SyncReplicasOptimizer(
							opt, replicas_to_aggregate=50,
							total_num_replicas=50)
						train = opt.minimize(
							total_loss, global_step = self.global_step)
						sync_replicas_hook = opt.make_session_run_hook(is_chief)
						with training.MonitoredTrainingSession(
						        master = workers[worker_id].target,
						        is_chief = is_chief,
						        hooks = [sync_replicas_hook]) as mon_sess:
						    while not mon_sess.should_stop():
						        mon_sess.run(training_op)

	other notes:
		sess.run(tf.global_variables_intializer())
		on one of the servers
		other:
			while len(sess.run(tf.report_unintialized_variables())) > 0
			    sleep
